# -*- coding: utf-8 -*-
"""Classifier_without_class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XXwypas2rix4-dLHLnYiqFXudVjk_AHG
"""

import numpy as np
import shutil
import torchvision
from torchvision import transforms
import re
import pandas as pd
import os
import torch
from sklearn.model_selection import train_test_split
import torch.nn as nn
import torch.nn.functional as F
import math
import pdb
from typing import Type, Any, Callable, Union, List, Optional
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from torch import Tensor,optim
import matplotlib.pyplot as plt
from torchvision import models
from sklearn.metrics import recall_score
import random
from torchvision.transforms import Normalize
from torch.autograd import Variable
from discriminator import Discriminator
import argparse
from PIL import Image
from  generator_classification import augnet
import sys
from Dataloader.Dataloader import MonkeyPoxDataLoader,MonkeyPoxRandAugDataLoader
import warnings
warnings.filterwarnings("ignore")



parser = argparse.ArgumentParser()
parser.add_argument('--G', help='Provide Batch(noise+deform) or intensity or deform')
parser.add_argument('--R', help='Range')
# parser.add_argument('--NR', help='Intensity Range')
parser.add_argument('--LR', help='Learning Rate')
parser.add_argument('--B', help='Batch Size')
parser.add_argument('--E', help='Epochs')
args = parser.parse_args()

generators = "intensity"
gen_range = 60
learning_rate = 0.00001
batch_size = 6

params = {'batch_size': batch_size,
    'shuffle': True}
rootdir = '/usr/sci/scratch/Moksha/CS6190_project/'
csv_dir = os.path.join(rootdir, "data")
img_dir = "/usr/sci/scratch/Moksha/CS6190_project/OriginalImages/OriginalImages/Total_Data/"
scratchDir = './Results'

tr_csv_file = os.path.join(csv_dir, "trainMonkeypox.csv")
cv_csv_file = os.path.join(csv_dir, "cvMonkeypox.csv")
te_csv_file = os.path.join(csv_dir, "testMonkeypox.csv")
trans = transforms.Compose(
[transforms.ToTensor(), Normalize(mean=(0.485), std=(0.229))])
test_trans = transforms.Compose(
[transforms.ToTensor(), Normalize(mean=(0.485), std=(0.229))])
train = MonkeyPoxDataLoader(tr_csv_file, img_dir, transform=trans)
train_dataloader = torch.utils.data.DataLoader(train, **params)
train_dataloader_eval = torch.utils.data.DataLoader(
train, batch_size=1, shuffle=True)

total_step = len(train_dataloader)
cv = MonkeyPoxDataLoader(cv_csv_file, img_dir, transform=test_trans)

cv_dataloader = torch.utils.data.DataLoader(cv, **params)
cv_dataloader_eval = torch.utils.data.DataLoader(
cv, batch_size=1, shuffle=True)

test = MonkeyPoxDataLoader(te_csv_file, img_dir, transform=test_trans)
test_dataloader = torch.utils.data.DataLoader(
test, batch_size=1, shuffle=True)


torch.backends.cudnn.benchmark = True

use_cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if use_cuda else "cpu")

# weights = torch.tensor(imbalance).to(device)
criterion = nn.BCELoss()

def accuracy(out, labels):
    pred = torch.max(out)
    return torch.sum(pred==labels).item()
print_every = 10

FloatTensor = torch.cuda.FloatTensor if device else torch.FloatTensor
LongTensor = torch.cuda.LongTensor if device else torch.LongTensor


gan_loss = torch.nn.MSELoss()

color_generator = augnet("color", 0, gen_range).to(device)
discriminator = Discriminator().to(device)
class gradient_reverse(nn.Module):
    
    def __init__(self):
        super(gradient_reverse, self).__init__()
        self.a = 1
    
    def forward(self,x):
        return x*self.a

    def backward(self,x):
        return x*-self.a
grad_reverse = gradient_reverse()
n_epochs = 1
def Resnet34(noise_dimension=16,lr = 0.00001, weight_decay=0.1, lambda_=1, gamma = 0.1):
    # optimizer_C = optim.AdamW(net.parameters(), lr=lr, weight_decay=weight_decay)
    best_train_recall = []
    best_cv_recall = []
    best_test_recall = []
    best_test_loss = [] 
    best_cv_loss = []
    best_train_loss = []
    best_test_acc = []
    best_cv_acc = []
    best_train_acc = [] 
    
    net = models.resnet34(pretrained=True).to(device)
    num_ftrs = net.fc.in_features
    net.fc = nn.Linear(num_ftrs, 1)
    net.fc = net.fc.cuda() if use_cuda else net.fc
    
    optimizer_C = torch.optim.Adam(color_generator.parameters(), lr=0.00001, weight_decay=weight_decay)
    C_gen_loss = []
    overall_noise_reg_loss = []
    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.00001, weight_decay=weight_decay)
    classifier_params = list(net.parameters()) + list(grad_reverse.parameters())
    optimizer_Adv = optim.AdamW(classifier_params, lr=0.00001, weight_decay=weight_decay)
    # valid_loss_min = np.Inf
    valid_recall_min = 0
    min_acc_val = 0
    val_recall = []
    train_loss = []
    val_loss = []
    # gen_loss = []
    # C_gen_loss = []
    overall_gan_loss = []
    # overall_reg_loss = []
    # overall_noise_reg_loss = []
    dis_loss = []
    train_acc = []
    val_acc = []
    train_recall = []
    for epoch in range(1, n_epochs+1):
        running_loss = 0.0
        gen_running_loss = 0.0
        C_gen_running_loss = 0.0
        dis_running_loss = 0.0
        total_gan_loss = 0.0
        reg_running_loss = 0.0
        noise_running_loss = 0.0
        correct = 0
        total=0
        train_temp_target_recall = []
        train_temp_pred_recall = []
        cv_temp_target_recall = []
        cv_temp_pred_recall = []
        print(f'Epoch {epoch}\n')
        for batch_idx, (data_, target_) in enumerate(train_dataloader):
            
            C_pred = []
            net.train()
            color_generator.train()
            discriminator.train()
            data_, target_ = data_.to(device), target_.to(device)
            target_ = target_.float()
            batch_size = data_.shape[0]
            Noise_data_ = data_[:int(batch_size*0.8),:,:,:]
            Dis_data_ = data_[int(batch_size*0.8):,:,:,:]
            Dis_data_ = Dis_data_.type(FloatTensor).to(device)
            Noise_data_ = Noise_data_.type(FloatTensor).to(device)
            target_2 = target_
            target_noise = target_[:int(batch_size*0.8)]
            valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)
            fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)
        
            optimizer_C.zero_grad()
            C_noise = torch.randn(Noise_data_.size(0),noise_dimension).to(device)
            C_gen_images, C_delta, C_reg_loss = color_generator(C_noise,Noise_data_,target_noise,require_loss=True,require_delta=True)
            C_validity = discriminator(C_gen_images)
            C_g_loss = gan_loss(C_validity, valid)

            optimizer_D.zero_grad()
            validity_real = discriminator(Dis_data_)
            d_real_loss = gan_loss(validity_real, valid)

            #Noise
            C_validity_fake = discriminator(C_gen_images.detach())
            C_d_fake_loss = gan_loss(C_validity_fake, fake)

            C_d_loss = (d_real_loss + C_d_fake_loss) / 2
            d_loss = C_d_loss
            loss_gan = d_loss+C_g_loss
            loss_adv = 0
            C_loss_adv = 0
            optimizer_Adv.zero_grad()
            reverse_grad = grad_reverse(C_gen_images)
            C_outputs = net(reverse_grad)
            C_outputs = torch.sigmoid(C_outputs).squeeze()
            C_loss_adv = criterion(C_outputs, target_noise)

            total_loss = C_loss_adv + lambda_*loss_gan + gamma * C_reg_loss
            total_loss.backward()
            
            optimizer_C.step()

            optimizer_D.step()
            
            optimizer_Adv.step()
            outputs_cls = net(data_)
            outputs_cls = torch.sigmoid(outputs_cls).squeeze()
            loss_cls = criterion(outputs_cls, target_2)
            
            optimizer_Adv.zero_grad()
            
            loss_cls.backward()
            optimizer_Adv.step()

            C_gen_running_loss += C_g_loss.item()
            noise_running_loss += C_reg_loss.item()
            dis_running_loss += d_loss.item()
            total_gan_loss += loss_gan.item()
            running_loss += total_loss.item()

            # C_pred = C_outputs
            for i in C_outputs:
                if i>0.5:
                    C_pred.append(1)
                else:
                    C_pred.append(0)
            C_pred = torch.Tensor(C_pred).cuda()
            correct += torch.sum(C_pred==target_noise).item()

            C_temp = target_noise.detach().cpu().numpy().tolist()
            for t in C_temp:
                train_temp_target_recall.append(t)
            C_temp = C_pred.detach().cpu().numpy().tolist()
            for t in C_temp:
                # if t>0.5:
                train_temp_pred_recall.append(t)
                # else:
                    # train_temp_pred_recall.append(0.0)
            total += target_noise.size(0)

            if (batch_idx) % 20 == 0:
            
                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, intensity generator loss: {:4f}, discriminator loss: {:4f}, gan loss: {:4f}, intensity reg loss: {:4f}' 
                    .format(epoch, n_epochs, batch_idx, total_step, total_loss.item(), C_g_loss.item(), d_loss.item(), loss_gan.item(), C_reg_loss.item()))
        
        if epoch % 10 == 0 or epoch==1:
                newpath = "Generator_samples_intensity/iteration_"+str(epoch)+"/"
                if not os.path.exists(newpath):
                        os.makedirs(newpath)
                for i in range(C_gen_images.size(0)):
                        fig, ax = plt.subplots(1,3)
                        fig.set_size_inches(20,16)

                        inp = Noise_data_[i, :, :, :]
                        inp = inp.detach().cpu().numpy().transpose((1, 2, 0))
                        mean = np.array([0.485, 0.456, 0.406])
                        std = np.array([0.229, 0.224, 0.225])
                        inp = std * inp + mean
                        res_noise_gen = Image.fromarray((inp*255).astype(np.uint8))

                        C_noise_gen = C_gen_images[i,:,:,:]
                        mean_C_noise_gen = np.array([0.485, 0.456, 0.406])
                        std_C_noise_gen = np.array([0.229, 0.224, 0.225])
                        C_noise_gen = C_noise_gen.detach().cpu().numpy().transpose((1, 2, 0))
                        C_noise_gen = std_C_noise_gen * C_noise_gen + mean_C_noise_gen
                        C_res_noise_gen = Image.fromarray((C_noise_gen*255).astype(np.uint8))

                        original_gen =  C_noise_gen-inp
                        mean_original_gen = np.array([0.485, 0.456, 0.406])
                        std_original_gen = np.array([0.229, 0.224, 0.225])
                        original_gen = std_original_gen * original_gen + mean_original_gen
                        res_original_noise_gen = Image.fromarray((original_gen*255).astype(np.uint8))

                        img1 = ax[0].imshow(res_noise_gen)
                        plt.colorbar(img1, ax=ax[0])
                        img2 = ax[1].imshow(C_res_noise_gen)
                        plt.colorbar(img2, ax=ax[1])
                        img3 = ax[2].imshow(res_original_noise_gen,cmap='gray')
                        plt.colorbar(img3, ax=ax[2])
                        fig.savefig(os.path.join(newpath,"noise_"+str(i)+'.jpg'),transparent=True,bbox_inches='tight')
        train_loss.append(running_loss/total_step)
        C_gen_loss.append(C_gen_running_loss/total_step)
        overall_noise_reg_loss.append(noise_running_loss/total_step)
        dis_loss.append(dis_running_loss/total_step)
        overall_gan_loss.append(total_gan_loss/total_step)
        train_acc.append(100 * correct/total)
        train_recall.append(recall_score(train_temp_target_recall,train_temp_pred_recall,average='macro'))
        batch_loss = 0
        total_t=0
        correct_t=0
        final_pred_val = []
        final_targets_val = []
        with torch.no_grad():
            net.eval()
            for data_t, target_t in (cv_dataloader):
                pred_t = []
                data_t, target_t = data_t.to(device), target_t.to(device)
                target_t = target_t.float()
                outputs_t = net(data_t)
                outputs_t = torch.sigmoid(outputs_t).squeeze()
                loss_t = criterion(outputs_t, target_t)
                batch_loss += loss_t.item()
                # pred_t = torch.max(outputs_t)
                for i in outputs_t:
                    if i>0.5:
                        pred_t.append(1)
                    else:
                        pred_t.append(0)
                pred_t = torch.Tensor(pred_t).cuda()
                correct_t += torch.sum(pred_t==target_t).item()
                temp = target_t.detach().cpu().numpy().tolist()
                for t in temp:
                    cv_temp_target_recall.append(t)
                temp = pred_t.detach().cpu().numpy().tolist()
                for t in temp:
                    # if t>0.5:
                    cv_temp_pred_recall.append(t)
                    # else:
                        # cv_temp_pred_recall.append(0.0)
                total_t += target_t.size(0)
            val_loss.append(batch_loss/len(cv_dataloader))
            val_acc.append(100 * correct_t/total_t)
            val_recall.append(recall_score(cv_temp_target_recall,cv_temp_pred_recall,average='macro'))
        # network_learned = batch_loss < valid_loss_min
        val_recall_score = recall_score(cv_temp_target_recall,cv_temp_pred_recall,average='macro')
        acc_val = (100 * correct_t/total_t)
        # network_learned = val_recall_score > valid_recall_min\
        network_learned = acc_val > min_acc_val
        if network_learned:
            # valid_recall_min = val_recall_score
            min_acc_val = acc_val
            torch.save(net.state_dict(), f"{scratchDir}/best_resnet_"+".torch")
            torch.save(color_generator, f"{scratchDir}/best_color_generator_"+".torch")
            torch.save(discriminator, f"{scratchDir}/best_discriminator_"+".torch")
            print('Improvement-Detected, save-model')
        print("train recall", recall_score(train_temp_target_recall,train_temp_pred_recall,average='macro'))
        print("validation recall",recall_score(cv_temp_target_recall,cv_temp_pred_recall,average='macro'))
        print(f'\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')
        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\n')
    fig, ax = plt.subplots(2,3)
    fig.set_size_inches(20,16)
    ax[0][0].plot(list(range(1,n_epochs+1)),train_loss,label="train")
    ax[0][0].plot(list(range(1,n_epochs+1)),val_loss,label="validation")
    ax[0][0].legend()
    
    ax[0][1].plot(list(range(1,n_epochs+1)),C_gen_loss,label="Appearance Generator")
    ax[0][1].plot(list(range(1,n_epochs+1)),dis_loss,label="Discriminator")
    ax[0][1].legend()
    ax[0][2].plot(list(range(1,n_epochs+1)),overall_gan_loss,label="Gan")
    ax[0][2].legend()

    ax[1][0].plot(list(range(1,n_epochs+1)),np.zeros((n_epochs)))
    ax[1][1].plot(list(range(1,n_epochs+1)),overall_noise_reg_loss,label="Noise Regularizer")
    ax[1][1].legend()     
    ax[1][2].plot(list(range(1,n_epochs+1)),np.zeros((n_epochs)))
    fig.savefig(f"{scratchDir}/"+"ResNet34Loss_"+".png",transparent=True,bbox_inches='tight')

    plt.title("ResNet34 Recall curve")
    plt.plot(list(range(1,n_epochs+1)),train_recall,label="train")
    plt.plot(list(range(1,n_epochs+1)),val_recall,label="validation")
    plt.xlabel("epochs")
    plt.ylabel("Recall")
    plt.legend()
    plt.savefig(f"{scratchDir}/"+"ResNet34recall_"+".png")


    plt.title("ResNet34 accuracy curve")
    plt.plot(list(range(1,n_epochs+1)),train_acc,label="train")
    plt.plot(list(range(1,n_epochs+1)),val_acc,label="validation")
    plt.xlabel("epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.savefig(f"{scratchDir}/"+"ResNet34accuracy_"+".png")

    
    net = models.resnet34(pretrained=True).to(device)
    num_ftrs = net.fc.in_features
    net.fc = nn.Linear(num_ftrs, 1)
    net.fc = net.fc.cuda() if use_cuda else net.fc

    net.load_state_dict(torch.load(f"{scratchDir}/best_resnet_"+".torch"))
    running_loss = 0.0
    correct_t = 0
    batch_loss = 0
    final_pred_train = []
    final_targets_train = []
    total_t=0
    with torch.no_grad():
            net.eval()
            for data_t, target_t in (train_dataloader):
                pred_t = []
                data_t, target_t = data_t.to(device), target_t.to(device)
                target_t = target_t.float()
                outputs_t = net(data_t)
                outputs_t = torch.sigmoid(outputs_t).squeeze()
                loss_t = criterion(outputs_t, target_t)
                batch_loss += loss_t.item()
                # pred_t = torch.max(outputs_t)
                # pred_t = outputs_t

                for i in outputs_t:
                    if i>0.5:
                        pred_t.append(1)
                    else:
                        pred_t.append(0)
                pred_t = torch.Tensor(pred_t).cuda()

                correct_t += torch.sum(pred_t==target_t).item()
                temp = target_t.detach().cpu().numpy().tolist()
                for t in temp:
                    final_targets_train.append(t)
                temp = pred_t.detach().cpu().numpy().tolist()
                for t in temp:
                    # if t>0.5:
                    final_pred_train.append(t)
                    # else:
                        # final_pred_train.append(0.0)
                total_t += target_t.size(0)
            best_train_loss.append(batch_loss/len(train_dataloader))
            best_train_acc.append(100 * correct_t/total_t)
            best_train_recall.append(recall_score(final_targets_train,final_pred_train,average='macro')) 
            
    print("train recall score",best_train_recall[-1])
    batch_loss = 0
    total_t=0
    correct_t=0
    final_targets_val = []
    final_pred_val = []
    with torch.no_grad():
            net.eval()
            for data_t, target_t in (cv_dataloader):
                pred_t = []
                data_t, target_t = data_t.to(device), target_t.to(device)
                target_t = target_t.float()
                outputs_t = net(data_t)
                outputs_t = torch.sigmoid(outputs_t).squeeze()
                loss_t = criterion(outputs_t, target_t)
                batch_loss += loss_t.item()
                # pred_t = torch.max(outputs_t)
                # pred_t = outputs_t

                for i in outputs_t:
                    if i>0.5:
                        pred_t.append(1)
                    else:
                        pred_t.append(0)
                pred_t = torch.Tensor(pred_t).cuda()
                
                correct_t += torch.sum(pred_t==target_t).item()
                temp = target_t.detach().cpu().numpy().tolist()
                for t in temp:
                    final_targets_val.append(t)
                temp = pred_t.detach().cpu().numpy().tolist()
                for t in temp:
                    # if t>0.5:
                    final_pred_val.append(t)
                    # else:
                        # final_pred_val.append(0.0)
                # final_targets_val.append(list(target_t.detach().cpu().numpy()))
                # final_pred_val.append(list(pred_t.detach().cpu().numpy()))
                total_t += target_t.size(0)
            best_cv_loss.append(batch_loss/len(cv_dataloader))
            best_cv_acc.append(100 * correct_t/total_t)
            best_cv_recall.append(recall_score(final_targets_val,final_pred_val,average='macro'))
            
    print("cv recall score",best_cv_recall[-1])
    batch_test_loss = 0
    test_loss = []
    test_acc = []
    correct = 0
    total_t = 0
    final_pred_test = []
    final_targets_test = []
    testRecallScore = 0
    orig = []
    pred = []
    with torch.no_grad():
        # net = torch.jit.load('resnet.pt')
        net.eval()
        for image,target in (test_dataloader):
            pred_t = []
            image, target = image.to(device), target.to(device)
            target = target.float()
            label = net(image)
            label = torch.sigmoid(label).squeeze(0)
            loss = criterion(label, target)
            batch_test_loss+=loss.item()
            # pred_t = torch.max(label)
            # pred_t = label

            for i in label:
                if i>0.5:
                    pred_t.append(1)
                else:
                    pred_t.append(0)
            for i in pred_t:
                pred.append(i)
            for i in target:
                orig.append(i.item())
            pred_t = torch.Tensor(pred_t).cuda()
            correct +=torch.sum(pred_t==target).item()
            temp = target.detach().cpu().numpy().tolist()
            for t in temp:
                final_targets_test.append(t)
            temp = pred_t.detach().cpu().numpy().tolist()
            for t in temp:
                # if t>0.5:
                final_pred_test.append(t)
                # else:
                    # final_pred_test.append(0.0)
            total_t += target.size(0)
        best_test_loss.append(batch_test_loss/len(test_dataloader))
        best_test_acc.append(100 * correct/total_t)
        best_test_recall.append(recall_score(final_targets_test,final_pred_test,average='macro'))
    cm = confusion_matrix(np.array(orig), np.array(pred))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()
    plt.savefig(os.path.join(scratchDir,"confusion_matrix_noise_aug_gan_resnet34.png"),transparent=True,bbox_inches='tight') 
    print(f'test loss: " {np.mean(best_test_loss[-1]):.4f}, test acc: {(100 * correct/total_t):.4f}\n')
    print("test recall score",best_test_recall[-1])

Resnet34(lr=learning_rate)


#Resnet34:
# train-loss: 0.4767, train-acc: 99.0291
# validation loss: 0.3731, validation acc: 82.8571

# train recall score 0.9941176470588236
# cv recall score 0.9473684210526316
# test loss: " 0.4592, test acc: 79.4872

# test recall score 0.7914438502673797
# resnet 34: confusion_matrix_noise_aug_gan_resnet34.png



# Resent50:

# train-loss: 0.4542, train-acc: 100.0000
# validation loss: 0.4153, validation acc: 80.0000

# train recall score 1.0
# cv recall score 0.9473684210526316
# test loss: " 0.5864, test acc: 82.0513

# resnet 50 : confusion_matrix_noise_aug_gan_resnet50
# test recall score 0.8141711229946524